# üîß Phase 1 Setup & Execution Guide

## **Complete Step-by-Step Implementation Instructions**

---

## üìã **Current Status Assessment**

### ‚úÖ **What You Already Have:**
1. Basic Airflow Docker setup
2. DVC initialized with DagHub remote
3. Node.js Express server (for dashboard)
4. Empty DVC-tracked dataset

### üÜï **What's Been Added:**
1. ‚úÖ Complete Airflow DAG with all 5 phases
2. ‚úÖ Requirements.txt with all Python dependencies
3. ‚úÖ MLflow service in docker-compose
4. ‚úÖ Environment configuration (.env.example)
5. ‚úÖ Proper directory structure
6. ‚úÖ Data quality gates
7. ‚úÖ Feature engineering
8. ‚úÖ Pandas profiling
9. ‚úÖ DVC automation

---

## üöÄ **Setup Instructions**

### **Step 1: Install Docker Dependencies in Airflow**

The Airflow containers need additional Python packages. Update your docker-compose.yaml:

**Option A: Build custom Airflow image (Recommended)**

Create `Dockerfile` in project root:

```dockerfile
FROM apache/airflow:2.7.1

USER root
RUN apt-get update && apt-get install -y git

USER airflow
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt
```

Update `docker-compose.yaml` - replace all `image: apache/airflow:2.7.1` with:
```yaml
build: .
```

**Option B: Install packages in running container (Quick test)**

```bash
# After starting containers
docker-compose exec airflow-scheduler pip install -r /opt/airflow/requirements.txt
docker-compose exec airflow-webserver pip install -r /opt/airflow/requirements.txt
```

---

### **Step 2: Configure DagHub Token**

1. **Get DagHub Token:**
   - Go to: https://dagshub.com/user/settings/tokens
   - Create new token with full access
   - Copy the token

2. **Create `.env` file:**
```bash
cp .env.example .env
```

3. **Edit `.env`** and add your token:
```bash
DAGSHUB_USERNAME=hamnariaz57
DAGSHUB_TOKEN=your_actual_token_here
```

4. **Configure DVC with credentials:**
```bash
dvc remote modify storage --local auth basic
dvc remote modify storage --local user hamnariaz57
dvc remote modify storage --local password YOUR_DAGSHUB_TOKEN
```

---

### **Step 3: Start Services**

```bash
# If using Option A (custom image)
docker-compose build

# Start all services
docker-compose up -d

# Monitor startup
docker-compose ps
docker-compose logs -f airflow-init

# Wait for "airflow-init exited with code 0"
```

**Services running:**
- Airflow Webserver: http://localhost:8080
- MLflow: http://localhost:5000
- PostgreSQL: port 5432

---

### **Step 4: Verify Setup**

```bash
# Check all containers are healthy
docker-compose ps

# Should show:
# airflow-webserver  -> healthy
# airflow-scheduler  -> running
# postgres          -> healthy
# mlflow            -> running

# Check Airflow can access Python packages
docker-compose exec airflow-scheduler python -c "import mlflow, dvc; print('OK')"
```

---

### **Step 5: Access Airflow & Enable DAG**

1. Open http://localhost:8080
2. Login: `admin` / `admin`
3. Find DAG: `exchange_rate_mlops_pipeline_phase1`
4. Toggle it **ON** (switch on left)
5. Click **Play button** ‚Üí "Trigger DAG" for manual run

---

### **Step 6: Monitor Execution**

**In Airflow UI:**
1. Click on DAG name
2. Click on latest run (Graph view)
3. Watch tasks turn green:
   - `extract_data` ‚Üí `quality_check` ‚Üí `transform_data` ‚Üí `generate_profiling_report` ‚Üí `version_with_dvc`

4. Click any task ‚Üí **Logs** to see detailed output

**Expected execution time:** 5-10 minutes (first run)

---

### **Step 7: Verify Outputs**

After successful run:

```bash
# Check raw data
ls -lh data/raw/
# Should have: exchange_rates_raw_YYYYMMDD_HHMMSS.csv

# Check processed data
ls -lh data/processed/
# Should have: exchange_rates.csv and exchange_rates.csv.dvc

# Check reports
ls -lh reports/
# Should have: data_profile_YYYYMMDD_HHMMSS.html

# Check DVC status
dvc status
# Should show: up to date
```

---

### **Step 8: Verify DVC Versioning**

```bash
# Check DVC tracking
dvc list --dvc-only .

# Check remote storage
dvc pull  # Should say "Everything is up to date"

# View Git commits
git log --oneline -5
# Should show: "Data version update - TIMESTAMP"
```

---

### **Step 9: Check MLflow Logs**

1. Open http://localhost:5000
2. Navigate to "Experiments"
3. Find run: `data_pipeline_TIMESTAMP`
4. Check:
   - **Parameters**: timestamp, num_rows, num_features
   - **Metrics**: quality_metrics
   - **Artifacts**: data_profile_*.html

**OR via DagHub:**
- https://dagshub.com/hamnariaz57/Mlops_Project.mlflow

---

## üîÑ **Daily Automated Execution**

The DAG runs **automatically every day at midnight UTC**.

To change schedule:
```python
# In dags/exchange_rate_dag.py
schedule_interval="@daily"     # Current: midnight UTC
# schedule_interval="0 9 * * *"  # 9 AM daily
# schedule_interval="@hourly"    # Every hour
```

---

## üêõ **Common Issues & Solutions**

### **Issue 1: Import errors in Airflow**

```
ModuleNotFoundError: No module named 'ydata_profiling'
```

**Solution:**
```bash
# Install in containers
docker-compose exec airflow-scheduler pip install -r /opt/airflow/requirements.txt
docker-compose exec airflow-webserver pip install -r /opt/airflow/requirements.txt
docker-compose restart airflow-scheduler airflow-webserver
```

---

### **Issue 2: DVC push fails with authentication error**

```
ERROR: failed to push data to the cloud - Authentication failed
```

**Solution:**
```bash
# Inside container or locally
dvc remote modify storage --local auth basic
dvc remote modify storage --local user hamnariaz57
dvc remote modify storage --local password YOUR_DAGSHUB_TOKEN

# Test
dvc push -v
```

---

### **Issue 3: MLflow not logging**

```
Warning: Could not log to MLflow
```

**Solution:**
Check `.env` file has correct token:
```bash
# Edit docker-compose.yaml to load .env
# Add under scheduler and webserver services:
env_file:
  - .env

# Restart
docker-compose down
docker-compose up -d
```

---

### **Issue 4: Airflow can't access Git/DVC**

```
ERROR: git command not found
```

**Solution:**
Must use custom Dockerfile (Option A above) to install git in Airflow container.

---

### **Issue 5: Profiling report generation too slow**

In DAG file, change:
```python
profile = ProfileReport(
    df,
    minimal=True,  # ‚Üê Faster generation
    explorative=True
)
```

---

## üìä **Understanding the Data Flow**

### **Day 1 (First Run):**
```
API ‚Üí Raw CSV (timestamped)
  ‚Üì
Quality Check (pass/fail)
  ‚Üì
Feature Engineering (basic - no historical data yet)
  ‚Üì
Processed CSV ‚Üí DVC version 1
  ‚Üì
Profiling Report ‚Üí MLflow artifact
```

### **Day 2+ (With History):**
```
API ‚Üí Raw CSV (timestamped)
  ‚Üì
Quality Check (pass/fail)
  ‚Üì
Feature Engineering (lag, rolling, volatility calculated from history)
  ‚Üì
Processed CSV ‚Üí DVC version 2 (updated)
  ‚Üì
Profiling Report ‚Üí MLflow artifact
```

---

## üß™ **Testing the Pipeline**

### **Manual Test Run:**

```bash
# Trigger DAG manually
docker-compose exec airflow-scheduler airflow dags trigger exchange_rate_mlops_pipeline_phase1

# Check status
docker-compose exec airflow-scheduler airflow dags state exchange_rate_mlops_pipeline_phase1

# View logs
docker-compose logs -f airflow-scheduler
```

---

## üìà **What to Submit for Phase 1**

### **Deliverables Checklist:**

- [ ] **Code**:
  - ‚úÖ `dags/exchange_rate_dag.py` (complete ETL pipeline)
  - ‚úÖ `requirements.txt`
  - ‚úÖ `docker-compose.yaml`
  - ‚úÖ `.dvc/config` (remote storage configured)

- [ ] **Data**:
  - ‚úÖ `data/processed/exchange_rates.csv.dvc` (DVC metadata in Git)
  - ‚úÖ Actual data in DagHub S3 remote

- [ ] **Reports**:
  - ‚úÖ Sample `data_profile_*.html` (Pandas Profiling)
  - ‚úÖ MLflow run showing logged artifacts

- [ ] **Documentation**:
  - ‚úÖ README.md with architecture & setup
  - ‚úÖ This implementation guide

- [ ] **Screenshots**:
  - Airflow UI showing successful DAG run (all tasks green)
  - MLflow UI showing experiment with artifacts
  - DagHub showing DVC tracked data

---

## üéØ **Phase 1 Validation Criteria**

| Requirement | How to Verify | Status |
|-------------|---------------|--------|
| **API data extraction** | Check `data/raw/` has timestamped files | ‚úÖ |
| **Quality gate implemented** | Trigger with bad data ‚Üí DAG fails | ‚úÖ |
| **Feature engineering** | Processed CSV has lag/rolling columns | ‚úÖ |
| **Pandas profiling** | HTML report in `reports/` | ‚úÖ |
| **MLflow artifact** | Report visible in MLflow UI | ‚úÖ |
| **DVC versioning** | `.dvc` file committed to Git | ‚úÖ |
| **Remote storage** | `dvc pull` downloads data | ‚úÖ |
| **Airflow orchestration** | Daily schedule works automatically | ‚úÖ |

---

## üöÄ **Next Actions**

1. **Run the pipeline** and verify all tasks succeed
2. **Take screenshots** of:
   - Airflow DAG run (graph view, all green)
   - MLflow experiment page
   - Profiling report opened
   - DVC status output

3. **Test quality gate** by modifying thresholds to intentionally fail
4. **Verify** data accumulates over multiple runs
5. **Document** any customizations you make

---

## üìû **Getting Help**

If stuck:

1. Check Airflow task logs (most detailed)
2. Run commands manually in scheduler container:
   ```bash
   docker-compose exec airflow-scheduler bash
   cd /opt/airflow
   python -c "from dags.exchange_rate_dag import *; extract_exchange_rate_data()"
   ```
3. Verify all environment variables are set
4. Check Docker container logs for startup errors

---

**You're ready for Phase 1 completion! üéâ**

