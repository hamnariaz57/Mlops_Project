# Phase IV: Monitoring and Observability - Implementation Complete âœ…

## Overview

Phase IV has been successfully implemented with full Prometheus and Grafana integration for monitoring your MLOps Exchange Rate Prediction API. This implementation provides comprehensive observability without disturbing the core application logic.

## What Was Implemented

### 1. âœ… Prometheus Integration in FastAPI Service

**File Modified**: `service.py`

**Changes Made**:
- Added `prometheus-fastapi-instrumentator` for automatic metrics collection
- Integrated `prometheus_client` for custom metrics
- Exposed `/metrics` endpoint automatically via instrumentator

**Metrics Implemented**:

#### Service Metrics:
1. **`http_requests_total`** (Counter)
   - Tracks total HTTP requests by method, endpoint, and status code
   - Labels: `method`, `endpoint`, `status`

2. **`inference_latency_seconds`** (Histogram)
   - Measures prediction request latency
   - Buckets: 1ms, 5ms, 10ms, 50ms, 100ms, 500ms, 1s, 5s
   - Enables p50, p95, p99 percentile calculations

#### Model/Data Drift Metrics:
3. **`data_drift_requests_total`** (Counter)
   - Counts requests with out-of-distribution feature values
   - Incremented when features are outside training distribution

4. **`data_drift_ratio`** (Gauge)
   - Current ratio of drift requests to total requests
   - Calculated in Grafana using PromQL: `rate(data_drift_requests_total[5m]) / rate(prediction_requests_total[5m])`

5. **`prediction_requests_total`** (Counter)
   - Total number of prediction requests

**Key Features**:
- Metrics are collected automatically on all endpoints
- Latency is measured for prediction requests
- Data drift detection compares incoming features against training statistics
- All metrics are exposed at `/metrics` endpoint

### 2. âœ… Training Data Statistics for Drift Detection

**File Modified**: `train_model.py`

**Changes Made**:
- Added automatic generation of training data statistics
- Saves statistics to `models/training_stats.json`
- Statistics include: min, max, mean, std for each feature (lag_1, lag_2, lag_3)

**Drift Detection Logic**:
- Checks if feature values are outside training min/max range
- Checks if values are > 3 standard deviations from training mean
- Flags requests as "drift" if any feature is out-of-distribution

### 3. âœ… Prometheus Configuration

**File Created**: `prometheus/prometheus.yml`

**Configuration**:
- Scrapes FastAPI service every 10 seconds
- Retains metrics for 30 days
- Configured to scrape from `model-api:8000/metrics`
- Includes Prometheus self-monitoring

### 4. âœ… Docker Compose for Monitoring Stack

**File Created**: `docker-compose.monitoring.yml`

**Services**:
1. **model-api**: FastAPI prediction service
   - Port: 8000
   - Exposes metrics endpoint
   - Mounts models directory for training statistics

2. **prometheus**: Metrics collection and storage
   - Port: 9090
   - Scrapes metrics from FastAPI service
   - Persistent storage for 30-day retention

3. **grafana**: Visualization and alerting
   - Port: 3000
   - Pre-configured datasource (Prometheus)
   - Auto-loaded dashboard

**Network**: All services on `monitoring-network` for secure communication

### 5. âœ… Grafana Dashboard

**Files Created**:
- `grafana/provisioning/datasources/prometheus.yml`: Auto-configures Prometheus datasource
- `grafana/provisioning/dashboards/dashboard.yml`: Auto-loads dashboards
- `grafana/dashboards/mlops-monitoring.json`: Complete monitoring dashboard

**Dashboard Panels**:

1. **Key Metrics (Top Row)**:
   - Total HTTP Requests (rate)
   - Inference Latency p95 (with color thresholds)
   - Data Drift Ratio (with color thresholds)
   - Total Prediction Requests

2. **Time Series Charts**:
   - Inference Latency Over Time (p50, p95, p99)
   - HTTP Request Rate by Endpoint
   - Data Drift Detection Over Time

3. **Distribution Charts**:
   - HTTP Status Codes Distribution (pie chart)
   - Latency Distribution (histogram)

**Features**:
- Auto-refresh every 10 seconds
- Color-coded thresholds for alerts
- Real-time visualization
- Responsive layout

### 6. âœ… Alerting Configuration

**File Created**: `grafana/alerts/alert-rules.json` (reference)

**Alerts Configured**:

1. **High Inference Latency Alert**:
   - **Condition**: p95 latency > 500ms for 5 minutes
   - **Severity**: Warning
   - **Description**: Indicates performance degradation

2. **Data Drift Spike Alert**:
   - **Condition**: Drift ratio > 30% for 5 minutes
   - **Severity**: Critical
   - **Description**: Indicates model performance may be degraded

**Alert Setup**:
- Alerts can be configured via Grafana UI
- Supports notification channels (Slack, Email, etc.)
- Detailed documentation in `MONITORING_SETUP.md`

### 7. âœ… Documentation and Scripts

**Files Created**:
- `MONITORING_SETUP.md`: Comprehensive setup and usage guide
- `start-monitoring.sh`: Bash script to start monitoring stack
- `start-monitoring.ps1`: PowerShell script for Windows
- `PHASE4_MONITORING_COMPLETE.md`: This file

## File Structure

```
Mlops_Project/
â”œâ”€â”€ service.py                          # âœ… Modified: Added Prometheus metrics
â”œâ”€â”€ train_model.py                      # âœ… Modified: Generates training_stats.json
â”œâ”€â”€ requirements.txt                    # âœ… Modified: Added prometheus-fastapi-instrumentator
â”œâ”€â”€ docker-compose.monitoring.yml       # âœ… New: Monitoring stack
â”œâ”€â”€ start-monitoring.sh                 # âœ… New: Startup script (Linux/Mac)
â”œâ”€â”€ start-monitoring.ps1                # âœ… New: Startup script (Windows)
â”œâ”€â”€ MONITORING_SETUP.md                 # âœ… New: Complete documentation
â”œâ”€â”€ prometheus/
â”‚   â””â”€â”€ prometheus.yml                  # âœ… New: Prometheus configuration
â””â”€â”€ grafana/
    â”œâ”€â”€ provisioning/
    â”‚   â”œâ”€â”€ datasources/
    â”‚   â”‚   â””â”€â”€ prometheus.yml          # âœ… New: Auto-configure datasource
    â”‚   â””â”€â”€ dashboards/
    â”‚       â””â”€â”€ dashboard.yml           # âœ… New: Dashboard provisioning
    â”œâ”€â”€ dashboards/
    â”‚   â””â”€â”€ mlops-monitoring.json       # âœ… New: Monitoring dashboard
    â””â”€â”€ alerts/
        â””â”€â”€ alert-rules.json            # âœ… New: Alert definitions (reference)
```

## Quick Start Guide

### Prerequisites

1. **Docker and Docker Compose** installed
2. **Trained model** available in `models/rf_model.pkl`
3. **Training statistics** in `models/training_stats.json` (generated by `train_model.py`)

### Step 1: Generate Training Statistics (if not done)

```bash
python train_model.py
```

This creates `models/training_stats.json` with feature statistics for drift detection.

### Step 2: Start Monitoring Stack

**Linux/Mac**:
```bash
chmod +x start-monitoring.sh
./start-monitoring.sh
```

**Windows (PowerShell)**:
```powershell
.\start-monitoring.ps1
```

**Or manually**:
```bash
docker-compose -f docker-compose.monitoring.yml up -d
```

### Step 3: Access Services

- **FastAPI Service**: http://localhost:8000
  - Health: http://localhost:8000/health
  - Metrics: http://localhost:8000/metrics
  - API Docs: http://localhost:8000/docs

- **Prometheus**: http://localhost:9090
  - Query metrics, view targets, check configuration

- **Grafana**: http://localhost:3000
  - Username: `admin`
  - Password: `admin` (change in production!)
  - Dashboard: Automatically loaded

### Step 4: Test the Setup

```bash
# Make a prediction request
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"history": [0.85, 0.86, 0.87]}'

# Check metrics endpoint
curl http://localhost:8000/metrics
```

### Step 5: View Dashboard

1. Open Grafana: http://localhost:3000
2. Login with admin/admin
3. Navigate to **Dashboards** â†’ **MLOps Exchange Rate API - Monitoring Dashboard**
4. Watch metrics update in real-time!

## Key Design Decisions

### Why This Approach?

1. **Non-Invasive Integration**:
   - Used `prometheus-fastapi-instrumentator` for automatic instrumentation
   - Minimal changes to existing code
   - Core logic remains untouched

2. **Comprehensive Metrics**:
   - Service metrics (latency, request count) for operational monitoring
   - Data drift metrics for model health monitoring
   - Both essential for production ML systems

3. **Standard Tools**:
   - Prometheus: Industry-standard metrics collection
   - Grafana: Powerful visualization and alerting
   - Docker Compose: Easy deployment and management

4. **Production-Ready**:
   - Persistent storage for metrics
   - Health checks and restart policies
   - Network isolation for security
   - Comprehensive documentation

### Data Drift Detection Strategy

**Why This Method?**:
- Simple and effective: Compare against training distribution
- Statistically sound: Uses z-score and range checks
- Lightweight: No external dependencies
- Real-time: Detects drift on every request

**Limitations**:
- Basic approach (can be enhanced with more sophisticated methods)
- Requires training statistics to be available
- Single-threshold detection (can be tuned)

**Future Enhancements**:
- Statistical tests (KS test, PSI)
- Feature importance weighting
- Time-based drift windows
- Model performance correlation

## Metrics Explained

### Service Metrics

**`http_requests_total`**:
- **What**: Total HTTP requests received
- **Why**: Monitor API usage and traffic patterns
- **Use**: Track request rates, identify popular endpoints

**`inference_latency_seconds`**:
- **What**: Time taken for prediction requests
- **Why**: Monitor performance and SLA compliance
- **Use**: Alert if latency exceeds 500ms threshold

### Data Drift Metrics

**`data_drift_requests_total`**:
- **What**: Count of requests with OOD features
- **Why**: Detect when model receives unexpected data
- **Use**: Alert if drift ratio spikes (indicates model degradation risk)

**`data_drift_ratio`**:
- **What**: Ratio of drift requests to total requests
- **Why**: Normalized metric for drift monitoring
- **Use**: Track drift trends over time

## Alerting Setup

### Configure Alerts in Grafana

1. **Navigate to Alerting**:
   - Go to **Alerting** â†’ **Alert rules** in Grafana

2. **Create High Latency Alert**:
   - Query: `histogram_quantile(0.95, rate(inference_latency_seconds_bucket[5m])) * 1000`
   - Condition: `WHEN last() OF A IS ABOVE 500`
   - For: `5m`

3. **Create Data Drift Alert**:
   - Query: `rate(data_drift_requests_total[5m]) / rate(prediction_requests_total[5m])`
   - Condition: `WHEN last() OF A IS ABOVE 0.3`
   - For: `5m`

4. **Add Notification Channel**:
   - Configure Slack, Email, or other channels
   - Add to alert rules for automatic notifications

See `MONITORING_SETUP.md` for detailed instructions.

## Verification Checklist

- [x] Prometheus dependencies added to requirements.txt
- [x] FastAPI service exposes /metrics endpoint
- [x] Service metrics (latency, request count) collected
- [x] Data drift metrics implemented
- [x] Training statistics generation added
- [x] Prometheus configuration created
- [x] Grafana dashboard created with all metrics
- [x] Alerting configuration documented
- [x] Docker Compose setup complete
- [x] Documentation comprehensive
- [x] Startup scripts provided

## Testing

### Test Metrics Collection

```bash
# 1. Start services
docker-compose -f docker-compose.monitoring.yml up -d

# 2. Make some requests
for i in {1..10}; do
  curl -X POST http://localhost:8000/predict \
    -H "Content-Type: application/json" \
    -d '{"history": [0.85, 0.86, 0.87]}'
  sleep 1
done

# 3. Check metrics
curl http://localhost:8000/metrics | grep -E "(http_requests_total|inference_latency|data_drift)"

# 4. Verify in Prometheus
# Open http://localhost:9090 and query: rate(http_requests_total[5m])

# 5. Verify in Grafana
# Open http://localhost:3000 and check dashboard
```

### Test Data Drift Detection

```bash
# Normal request (should not trigger drift)
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"history": [0.85, 0.86, 0.87]}'

# Extreme values (may trigger drift if outside training range)
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"history": [10.0, 20.0, 30.0]}'

# Check drift metric
curl http://localhost:8000/metrics | grep data_drift
```

## Troubleshooting

### Common Issues

1. **Metrics not appearing**:
   - Check API is running: `docker logs exchange-rate-api`
   - Check Prometheus targets: http://localhost:9090/targets
   - Verify network: `docker exec prometheus wget -qO- http://model-api:8000/metrics`

2. **Training statistics not found**:
   - Run: `python train_model.py`
   - Copy: `cp training_stats.json models/`
   - Restart: `docker-compose -f docker-compose.monitoring.yml restart model-api`

3. **Dashboard not loading**:
   - Check file exists: `ls grafana/dashboards/mlops-monitoring.json`
   - Check Grafana logs: `docker logs grafana`
   - Manually import via Grafana UI

See `MONITORING_SETUP.md` for detailed troubleshooting.

## Production Considerations

### Security

1. **Change default passwords**:
   - Update Grafana admin password
   - Use environment variables or secrets management

2. **Network security**:
   - Use internal Docker networks (already configured)
   - Expose only necessary ports
   - Consider reverse proxy for external access

3. **Access control**:
   - Configure Grafana user roles
   - Limit Prometheus access
   - Use authentication for API

### Performance

1. **Retention policy**:
   - Adjust `storage.tsdb.retention.time` in Prometheus
   - Current: 30 days (adjust based on needs)

2. **Scrape intervals**:
   - Balance real-time monitoring vs resource usage
   - Current: 10s for API (good for real-time)

3. **Resource limits**:
   - Add resource limits to Docker Compose
   - Monitor resource usage

### High Availability

For production, consider:
- Prometheus federation for scaling
- Grafana high availability
- Alertmanager for alert routing
- Backup and restore procedures

## Next Steps

1. **Customize alerts** for your specific thresholds
2. **Add notification channels** (Slack, PagerDuty, etc.)
3. **Create additional dashboards** for different stakeholders
4. **Implement log aggregation** (ELK stack) for comprehensive observability
5. **Add more metrics** as needed (model accuracy, feature importance, etc.)

## Summary

âœ… **Phase IV is complete!** Your MLOps system now has:

- âœ… Prometheus metrics collection
- âœ… Real-time monitoring dashboard
- âœ… Data drift detection
- âœ… Alerting capabilities
- âœ… Production-ready setup
- âœ… Comprehensive documentation

The implementation is **non-invasive**, **production-ready**, and **fully documented**. All core logic remains untouched, and the monitoring stack can be started independently.

## Support

For detailed usage instructions, see `MONITORING_SETUP.md`.

For issues:
1. Check logs: `docker-compose -f docker-compose.monitoring.yml logs`
2. Verify configuration files
3. Test individual services
4. Review documentation

---

**Phase IV Implementation Complete** âœ…
**Ready for Production Monitoring** ðŸš€

